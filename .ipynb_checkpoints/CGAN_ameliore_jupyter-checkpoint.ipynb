{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import sys\n",
    "from os.path import isfile\n",
    "from os import remove\n",
    "\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from keras_preprocessing.image import load_img\n",
    "from keras_preprocessing.image import img_to_array\n",
    "from os import listdir\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(keras.__version__)\n",
    "\n",
    "########################################\n",
    "########## Gestion des images ##########\n",
    "########################################\n",
    "\n",
    "def compress_images(dim):\n",
    "    \"\"\"\n",
    "    Cette fonction prend toutes les images en .jpg, les import, les transformes en tableau, puis stock le gros tableau résultant\n",
    "    pour qu'il soit plus simple à charger la prochaine fois\n",
    "    \"\"\"\n",
    "\n",
    "    dataA = load_images(\"TrainFaces/\", size=(dim,dim))\n",
    "    dataB = load_images(\"TrainManga/\", size=(dim,dim))\n",
    "\n",
    "    np.savez_compressed(\"f2m.npz\", dataA, dataB)\n",
    "    print(\"dataset saved as f2m.npz\" )\n",
    "\n",
    "def load_compressed_images():\n",
    "    \"\"\"\n",
    "    Charge les images depuis la version compressée fabriquée plus haut\n",
    "    \"\"\"\n",
    "    data = np.load('f2m.npz')\n",
    "    print(\"{} loaded\".format(str(data['arr_0'].shape)))\n",
    "    print(\"{} loaded\".format(str(data['arr_1'].shape)))\n",
    "    return (data['arr_0'], data['arr_1'])\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Charge les images et les convertis entre -1 et 1 pour être bien utilisée dans la suite \"\"\"\n",
    "    XA,XB = load_compressed_images()\n",
    "    XA = XA/127.5-1\n",
    "    XB = XB/127.5-1\n",
    "    return XA,XB\n",
    "\n",
    "def show_images(dataA, dataB, titleA = [], titleB = []):\n",
    "    # plot source images\n",
    "    for i in range(dataA.shape[0]):\n",
    "        if len(titleA) == 0:\n",
    "            plt.subplot(2, dataA.shape[0], 1 + i)\n",
    "        else:\n",
    "            plt.subplot(2, dataA.shape[0], 1 + i, title=str(titleA[i]))\n",
    "        plt.axis('off')\n",
    "        plt.imshow(dataA[i].astype('uint8'))\n",
    "    # plot target image\n",
    "    for i in range(dataB.shape[0]):\n",
    "        if len(titleB) == 0:\n",
    "            plt.subplot(2, dataB.shape[0], 1 + dataA.shape[0] + i)\n",
    "        else:\n",
    "            plt.subplot(2, dataB.shape[0], 1 + dataA.shape[0] + i,title=str(titleB[i]))\n",
    "        \n",
    "        plt.axis('off')\n",
    "        plt.imshow(dataB[i].astype('uint8'))\n",
    "    plt.show()\n",
    "\n",
    "def save_images(dataA, dataB, filename):\n",
    "    # plot source images\n",
    "    for i in range(dataA.shape[0]):\n",
    "        plt.subplot(2, dataA.shape[0], 1 + i)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(dataA[i].astype('uint8'))\n",
    "    # plot target image\n",
    "    for i in range(dataB.shape[0]):\n",
    "        plt.subplot(2, dataB.shape[0], 1 + dataA.shape[0] + i)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(dataB[i].astype('uint8'))\n",
    "    if isfile(filename):\n",
    "        remove(filename)\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "def load_images(path, size):\n",
    "    \"\"\"\n",
    "    Charge les images d'un dossier\n",
    "    \"\"\"\n",
    "    data_list = list()\n",
    "    for filename in tqdm(listdir(path)):\n",
    "        pixels = load_img(path + filename, target_size=size)\n",
    "        pixels = img_to_array(pixels)\n",
    "        data_list.append(pixels)\n",
    "    return np.asarray(data_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "########## Création du réseau ##########\n",
    "########################################\n",
    "\n",
    "def create_discriminator(dim, depht = 32, name=\"\", learning_factor = 1):\n",
    "    \"\"\"\n",
    "    On change la structure par / à CGAN.py, voir pdf page 6 figure 2\n",
    "    \"\"\"\n",
    "    input_layer = keras.layers.Input(shape=(dim,dim,3))\n",
    "    #Layer 1 : Convolution avec un filtre de 4x4 qui se déplace de 2 pixels en 2 -> Division du nombre de pixel par 2; depht filtres utilisés\n",
    "    #On ajoute un InstanceNormalization pour réduire les poids et éviter une explosion du gradient\n",
    "    #1] Conv; dim*dim*3 -> dim/2*dim/2*2depht\n",
    "    d = keras.layers.Conv2D(2*depht, (4,4), strides=(2,2), padding=\"same\")(input_layer)\n",
    "    d = InstanceNormalization(axis=-1)(d)\n",
    "    d = keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    #2] Conv; dim/2*dim/2*depht -> dim/4*dim/4*4*depht\n",
    "    d = keras.layers.Conv2D(4*depht, (4,4), strides=(2,2), padding=\"same\")(d)\n",
    "    d = InstanceNormalization(axis=-1)(d)\n",
    "    d = keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    #3] Conv; dim/4*dim/4*2*depht -> dim/8*dim/8*8*depht\n",
    "    d = keras.layers.Conv2D(8*depht, (4,4), strides=(2,2), padding=\"same\")(d)\n",
    "    d = InstanceNormalization(axis=-1)(d)\n",
    "    d = keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    #4] Conv; dim/8*dim/8*8*depht -> dim/8*dim/8*8*depht\n",
    "    d = keras.layers.Conv2D(8*depht, (3,3), strides=(1,1), padding=\"same\")(d)\n",
    "    d = InstanceNormalization(axis=-1)(d)\n",
    "    pre_dil_conv = keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    #C'est ici que se trouve un premier skip d'après le papier, on continue donc de l'autre coté avec des reseau de convolutions\n",
    "    #dilués et l'on ferra une concatenation plus loin pour permettre la connection\n",
    "\n",
    "    #5] Dil Conv de d = 2\n",
    "    d = keras.layers.Conv2D(8*depht, (3,3), strides=(1,1), dilation_rate=(2,2),padding=\"same\")(pre_dil_conv)\n",
    "    d = InstanceNormalization(axis=-1)(d)\n",
    "    d = keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    #6] Dil Conv de d = 4\n",
    "    d = keras.layers.Conv2D(8*depht, (3,3), strides=(1,1), dilation_rate=(4,4),padding=\"same\")(d)\n",
    "    d = InstanceNormalization(axis=-1)(d)\n",
    "    d = keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    #7] Dil Conv de d = 8\n",
    "    d = keras.layers.Conv2D(8*depht, (3,3), strides=(1,1), dilation_rate=(8,8),padding=\"same\")(d)\n",
    "    d = InstanceNormalization(axis=-1)(d)\n",
    "    post_dil_conv = keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    #8] Reconnection par concatenation\n",
    "    d = keras.layers.Concatenate()([pre_dil_conv, post_dil_conv])\n",
    "\n",
    "    #9] Fin du réseau : Conv\n",
    "    d = keras.layers.Conv2D(8*depht, (3,3), strides=(1,1), padding=\"same\")(d)\n",
    "    d = InstanceNormalization(axis=-1)(d)\n",
    "    d = keras.layers.LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    #10] Dernier conv pour avoir un vecteur\n",
    "    d = keras.layers.Conv2D(1, (4,4), strides=(1,1), padding=\"same\")(d)\n",
    "\n",
    "    #On compile\n",
    "    model = keras.Model(input_layer, d)\n",
    "    opt = keras.optimizers.Adam(lr=0.0002*learning_factor, beta_1=0.5)\n",
    "    model.compile(loss='mse', optimizer=opt, loss_weights=[0.5], metrics=[\"accuracy\"])\n",
    "\n",
    "    #Enfin, on enregistre dans un fichier si jamais c'est demandé pour vérifier la structure du réseau\n",
    "    #plot_model(d, to_file=\"d_{}.png\".format(name), show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_generator(dim, depht = 32, name=\"\"):\n",
    "    \"\"\"    On change la structure par / à CGAN.py, voir pdf \"\"\"\n",
    "    input_layer = keras.layers.Input(shape=(dim,dim,3))\n",
    "\n",
    "    #1) Convolution (dim,dim,3) -> (dim/2,dim/2,depht)\n",
    "    g = keras.layers.Conv2D(depht, (4,4), strides=(2,2), padding=\"same\")(input_layer)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = keras.layers.LeakyReLU(alpha=0.2)(g)\n",
    "\n",
    "    #2) Convolution (dim/2,dim/2,depht) -> (dim/2,dim/2,4*depht)\n",
    "    g = keras.layers.Conv2D(4*depht, (4,4), strides=(1,1), padding=\"same\")(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = keras.layers.LeakyReLU(alpha=0.2)(g)\n",
    "\n",
    "    #3) 3 RESNET : (dim/2,dim/2,4*depht)\n",
    "    g = create_resnet(g)\n",
    "    g = create_resnet(g)\n",
    "    point_1 = create_resnet(g)\n",
    "\n",
    "    #4) Convolution : (dim/2,dim/2,4*depht) -> (dim/4,dim/4,8*depht)\n",
    "    g = keras.layers.Conv2D(8*depht, (4,4), strides=(2,2), padding=\"same\")(point_1)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = keras.layers.LeakyReLU(alpha=0.2)(g)\n",
    "\n",
    "    #4) 3 Resnet : (dim/4,dim/4,8*depht)\n",
    "    g = create_resnet(g)\n",
    "    g = create_resnet(g)\n",
    "    point_2 = create_resnet(g)\n",
    "\n",
    "    #5) Convolution (dim/4,dim/4,8*depht) -> (dim/8,dim/8,8*depht)\n",
    "    g = keras.layers.Conv2D(8*depht, (4,4), strides=(2,2), padding=\"same\")(point_2)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = keras.layers.LeakyReLU(alpha=0.2)(g)\n",
    "\n",
    "    #3) RESNET\n",
    "    for _ in range(3):\n",
    "        g = create_resnet(g)\n",
    "\n",
    "    #4) Deconv : (dim/8,dim/8,8*depht) -> (dim/4,dim/4,8*depht)\n",
    "    g = keras.layers.Conv2DTranspose(8*depht, (3,3), strides=(2,2), padding=\"same\")(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = keras.layers.LeakyReLU(alpha=0.2)(g)\n",
    "\n",
    "    #Raccord 2 : (dim/4,dim/4,8*depht) + (dim/4,dim/4,8*depht) -> (dim/4,dim/4,16*depht)\n",
    "    g = keras.layers.Concatenate()([g, point_2])\n",
    "\n",
    "    #3 RESNET (dim/4,dim/4,16*depht)\n",
    "    for _ in range(3):\n",
    "        g = create_resnet(g)\n",
    "\n",
    "    #Transpose Conv : (dim/4,dim/4,16*depht) -> (dim/2,dim/2,4*depht)\n",
    "    g = keras.layers.Conv2DTranspose(4*depht, (4,4), strides=(2,2), padding=\"same\")(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = keras.layers.LeakyReLU(alpha=0.2)(g)\n",
    "\n",
    "    #Raccord 1 : (dim/2,dim/2,4*depht) + (dim/2,dim/2,4*depht) -> (dim/2,dim/2,8*depht)\n",
    "    g = keras.layers.Concatenate()([g, point_1])\n",
    "\n",
    "    #3) RESNET (dim/2,dim/2,8*depht)\n",
    "    for _ in range(3):\n",
    "        g = create_resnet(g)\n",
    "\n",
    "    #DeConvolution : (dim/2,dim/2,8*depht) -> (dim,dim,depht)\n",
    "    g = keras.layers.Conv2DTranspose(depht, (4,4), strides=(2,2), padding=\"same\")(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = keras.layers.LeakyReLU(alpha=0.2)(g)\n",
    "\n",
    "    #DeConvolution : (dim,dim,depht) -> (dim,dim,3)\n",
    "    g = keras.layers.Conv2DTranspose(3, (4,4), strides=(1,1), padding=\"same\")(g)\n",
    "    g = keras.layers.Activation(\"tanh\")(g)\n",
    "\n",
    "    M = keras.Model(input_layer, g, name=\"gen_{}\".format(name))\n",
    "    return M\n",
    "\n",
    "\n",
    "def create_resnet(T):\n",
    "    n_filters = T.shape[-1]\n",
    "    N = keras.layers.Conv2D(n_filters, (3,3), strides=(1,1), padding=\"same\")(T)\n",
    "    N = InstanceNormalization(axis=-1)(N)\n",
    "    N = keras.layers.LeakyReLU(alpha=0.2)(N)\n",
    "\n",
    "    N = keras.layers.Conv2D(n_filters, (3,3), strides=(1,1), padding=\"same\")(N)\n",
    "    N = InstanceNormalization(axis=-1)(N)\n",
    "\n",
    "    #On additionne l'entrée et la sortie (ce qui fait la particularité du RESNET)\n",
    "    #L'addition fonctionne forcément vu que le nombre de filtre correspond bien et que l'on a pas touché a la taille du reseau\n",
    "    N = keras.layers.Add()([N, T])\n",
    "\n",
    "    #Dernière fonction d'activation\n",
    "    N = keras.layers.LeakyReLU(alpha=0.2)(N)\n",
    "    return N\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "########## Création des structures d'entrainement ##########\n",
    "############################################################\n",
    "\n",
    "def create_training_model_gen(gen_1_vers_2, d_2, gen_2_vers_1, dim, name=\"\"):\n",
    "    \"\"\"\n",
    "    Cette méthode combine les différents réseau pour en déduire des fonctions de loss que nous allons chercher a minimiser\n",
    "    Ici, c'est seulement gen_1_vers_2 qui va être entrainé\n",
    "    \"\"\"\n",
    "\n",
    "    #On desactive tout sauf gen_1_vers_2\n",
    "    gen_1_vers_2.trainable = True\n",
    "    gen_2_vers_1.trainable = False\n",
    "    d_2.trainable = False\n",
    "\n",
    "    #Voila les deux entrees de ce model : une entree du monde 1 et une autre du monde 2\n",
    "    input_from_1 = keras.layers.Input(shape=(dim,dim,3))\n",
    "    input_from_2 = keras.layers.Input(shape=(dim,dim,3))\n",
    "\n",
    "    #Entrainement 1 : On veut que gen_1_vers_2 soit capable de tromper le discriminateur d2\n",
    "    #On entraine donc le reseau d_2(gen_1_vers_2(input_from_1)) en cherchant à obtenir 1 à chaque fois\n",
    "    pred_d2 = d_2(gen_1_vers_2(input_from_1))\n",
    "\n",
    "    #Entrainement 2 et 3 : L'objectif est que logiquement, gen_1_vers_2 = gen_2_vers_1^-1\n",
    "    #Donc on va s'entrainer sur deux boucles, gen_1_vers_2(gen_2_vers_1(input_from_2)) = input_from_2\n",
    "    # et dans l'autre sens gen_2_vers_1(gen_1_vers_2(input_1)) = input_1\n",
    "    cycle_1 = gen_2_vers_1(gen_1_vers_2(input_from_1))\n",
    "    cycle_2 = gen_1_vers_2(gen_2_vers_1(input_from_2))\n",
    "\n",
    "    #Entrainement 4 : Enfin, une image du monde 2 ne doit pas changer par gen_1_vers_2\n",
    "    identity_2 = gen_1_vers_2(input_from_2)\n",
    "\n",
    "    #Ces 4 entrainements sont mis ensemble pour etre tous traités en même temps\n",
    "    model = keras.Model([input_from_1, input_from_2], [pred_d2, cycle_1, cycle_2, identity_2],name=\"train_gen_{}\".format(name))\n",
    "    opt = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "    #Compilation du model, on va minimiser la CL de ces fonctions de pertes, pondéré par les poids en dessous\n",
    "    # (on donne plus d'importance aux cycles d'après le papier)\n",
    "    model.compile(loss=['mse', 'mae', 'mae', 'mae'], loss_weights=[1, 5, 5, 2], optimizer=opt, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "##################################\n",
    "########## Entrainement ##########\n",
    "##################################\n",
    "def get_random_element(X, n):\n",
    "    return X[np.random.randint(0,X.shape[0], n),...]\n",
    "\n",
    "def update_pool(existing_pool, new_images, pool_max_size=50):\n",
    "    \"\"\"\n",
    "    Cette fonction tient un historique des dernières images générée, car d'après le document il est plus performant\n",
    "    d'entrener le générateur sur des images précédemment générées.\"\"\"\n",
    "    selected = []\n",
    "    for img in new_images:\n",
    "        #Si le pool n'est pas encore rempli, on l'ajoute au pool\n",
    "        if (len(existing_pool) < pool_max_size):\n",
    "            existing_pool.append(img)\n",
    "            selected.append(img)\n",
    "        #Sinon une chance sur deux de l'utilisé, ou bien d'utilisé une vielle image\n",
    "        elif np.random.random() > 0.5 :\n",
    "            #On l'utilise mais on ne l'ajoute pas dans le pool\n",
    "            selected.append(img)\n",
    "        else:\n",
    "            #On utilise une ancienne image que l'on enleve du pool\n",
    "            index = np.random.randint(0,len(existing_pool))\n",
    "            selected.append(existing_pool[index])\n",
    "            existing_pool[index] = img\n",
    "    return np.asarray(selected)\n",
    "\n",
    "\n",
    "def train(  gen_A_vers_B, d_A, gen_B_vers_A, d_B, \n",
    "            training_model_gen_A_vers_B, training_model_gen_B_vers_A,  \n",
    "            XA, XB,\n",
    "            starting_epoch = 0):\n",
    "\n",
    "    \"\"\"C'est ici que se passe le gros entrainement\"\"\"\n",
    "    \n",
    "    #Caractéristiques de l'entrainement\n",
    "    n_epochs, n_batch, N_data = 1000, 2, max(XA.shape[0], XB.shape[0])\n",
    "    d_update_period = 1\n",
    "    bilans_period, time_point, bilan_index = 5*60, time(), starting_epoch #30mins\n",
    "    n_run_by_epochs = int(N_data/n_batch)\n",
    "    shape_y = (n_batch, d_A.output_shape[1], d_A.output_shape[2], d_A.output_shape[3])\n",
    "\n",
    "    poolA, poolB = [],[]\n",
    "    loss_gen_A_vers_B, loss_gen_B_vers_A = [],[]\n",
    "    loss_d_A, loss_d_B = [],[]\n",
    "\n",
    "    #Et la boucle qui tourne a tournée (ty Ribery)\n",
    "    while True:\n",
    "        #Construction du jeu de données a utiliser pour cette iteration de l'entrainement\n",
    "        xa_real, ya_real = get_random_element(XA, n_batch), np.ones(shape_y).astype(np.float32)\n",
    "        xb_real, yb_real = get_random_element(XB, n_batch), np.ones(shape_y).astype(np.float32)\n",
    "\n",
    "        xa_fake, ya_fake = update_pool(poolA, gen_B_vers_A.predict(xb_real)), np.zeros(shape_y).astype(np.float32)\n",
    "        xb_fake, yb_fake = update_pool(poolB, gen_A_vers_B.predict(xa_real)), np.zeros(shape_y).astype(np.float32)\n",
    "\n",
    "        #Entrainements\n",
    "        #1) On entraine gen_A_vers_B : ici, le monde 1 est A et le monde 2 est B\n",
    "        #on avait gen_1_vers_2 : [input_from_1, input_from_2] -> [pred_d2, cycle_1, cycle_2, identity_2]\n",
    "        e1 = training_model_gen_A_vers_B.train_on_batch([xa_real, xb_real], [yb_real, xa_real, xb_real, xb_real])\n",
    "        loss_gen_A_vers_B.append(np.array(e1))\n",
    "\n",
    "        #4) de même pour d_B\n",
    "        xb, yb = np.concatenate((xb_real, xb_fake)), np.concatenate((yb_real, yb_fake))\n",
    "        e4 = d_B.train_on_batch(xb, yb)\n",
    "        loss_d_B.append(np.array(e4))\n",
    "\n",
    "        #2) Sur le meme model, on entraine gen_B_vers_A\n",
    "        # gen_1_vers_2 : [input_from_1, input_from_2] -> [pred_d2, cycle_1, cycle_2, identity_2]\n",
    "        e2 = training_model_gen_B_vers_A.train_on_batch([xb_real, xa_real], [ya_real, xb_real, xa_real, xa_real])\n",
    "        loss_gen_B_vers_A.append(np.array(e2))\n",
    "\n",
    "        #3) On entraine d_A : input_from_A -> y\n",
    "        #On l'entraine a la fois avec des vrais données et des fausses\n",
    "        xa, ya = np.concatenate((xa_real, xa_fake)), np.concatenate((ya_real, ya_fake))\n",
    "        e3 = d_A.train_on_batch(xa, ya)\n",
    "        loss_d_A.append(np.array(e3))\n",
    "\n",
    "        #On check le temps écoulé et on fait un bilan si nécessaire\n",
    "        if (time() - time_point) > bilans_period:\n",
    "            time_point = time()\n",
    "            Bilan(  loss_d_A, loss_d_B, loss_gen_A_vers_B, loss_gen_B_vers_A, \n",
    "                    XA, XB, d_A, d_B, gen_A_vers_B, gen_B_vers_A, bilan_index)\n",
    "            bilan_index+=1\n",
    "            #On reset es grandeurs\n",
    "            loss_gen_A_vers_B, loss_gen_B_vers_A = [],[]\n",
    "            loss_d_A, loss_d_B = [],[]\n",
    "\n",
    "def Bilan(loss_d_A, loss_d_B, loss_gen_A_vers_B, loss_gen_B_vers_A, XA, XB, d_A, d_B, gen_A_vers_B, gen_B_vers_A, index):\n",
    "    #On affiche un petit résumé de la ou on en est\n",
    "    print(\"\")\n",
    "    print(\"#######################################\")\n",
    "    print(\"Bilan {}:\".format(index))\n",
    "    print(\"loss gen_A_vers_B : {}\".format(loss_info(avg(loss_gen_A_vers_B))))\n",
    "    print(\"loss gen_B_vers_A : {}\".format(loss_info(avg(loss_gen_B_vers_A))))\n",
    "    print(\"loss d_A : {}\".format(loss_info(avg(loss_d_A))))\n",
    "    print(\"loss d_B : {}\".format(loss_info(avg(loss_d_B))))\n",
    "    print(\"#######################################\")\n",
    "\n",
    "    screenshoot(XA, gen_A_vers_B, \"A_vers_B_\" + str(index))\n",
    "    screenshoot(XB, gen_B_vers_A, \"B_vers_A_\" + str(index))\n",
    "    \n",
    "    #On lache notre meilleure sauvegarde\n",
    "    save(d_A, d_B, gen_A_vers_B, gen_B_vers_A)\n",
    "\n",
    "def loss_info (loss) : \n",
    "    return [str(loss[i]) for i in range(loss.shape[0])]\n",
    "\n",
    "def avg(L):\n",
    "    if (len(L) == 0):\n",
    "        print(\"Erreur de moyenne, la liste est vide\")\n",
    "        return -1\n",
    "    return sum(L)/len(L)\n",
    "\n",
    "def train_discriminator_with_threshold(d, x_real, x_fake, y_real, y_fake, loss, d_accuracy_threshold = 0.80):\n",
    "    x, y = np.concatenate((x_real, x_fake)), np.concatenate((y_real, y_fake))\n",
    "    e = d.test_on_batch(x, y)\n",
    "    if (len(loss) == 0 or e[1] <= d_accuracy_threshold):\n",
    "        e = d.train_on_batch(x, y)\n",
    "    loss.append(np.array(e))\n",
    "\n",
    "def train_discriminator_with_period(d, x_real, x_fake, y_real, y_fake, loss, i, period = 1):\n",
    "    if i%period == 0:\n",
    "        x, y = np.concatenate((x_real, x_fake)), np.concatenate((y_real, y_fake))\n",
    "        e = d.train_on_batch(x, y)\n",
    "        loss.append(np.array(e))\n",
    "\n",
    "def train_discriminator(d, x_real, x_fake, y_real, y_fake, loss):\n",
    "    x, y = np.concatenate((x_real, x_fake)), np.concatenate((y_real, y_fake)) \n",
    "    loss.append(np.array(d.train_on_batch(x, y)))\n",
    "\n",
    "def train_generator(training_g_1_vers_2, x1_real, x2_real, y2_real, loss):\n",
    "    #on avait gen_1_vers_2 : [input_from_1, input_from_2] -> [pred_d2, cycle_1, cycle_2, identity_2]\n",
    "    e = training_g_1_vers_2.train_on_batch([x1_real, x2_real],[y2_real, x1_real, x2_real, x2_real])\n",
    "    loss.append(np.array(e))\n",
    "\n",
    "def screenshoot(X, gen, epoch):\n",
    "    \"\"\"Fait quelques tests et enregistre l'image pour voir la progression\"\"\"\n",
    "    data_id = [0,1705,115]\n",
    "    data1 = (X[data_id,...]+1)*127.5\n",
    "    data2 = (gen.predict(X[data_id,...])+1)*127.5\n",
    "    save_images(data1, data2, \"Progression/{}.png\".format(epoch))\n",
    "\n",
    "def show_result_network(X):\n",
    "    data = (X+1)*127.5\n",
    "    show_images(data, np.array([]))\n",
    "\n",
    "def test(img, gen, dcorrect, dautre):\n",
    "    #On va créer les sous titres\n",
    "    #Pour les img originales du monde correct\n",
    "    titresimg, titrestransf = [],[]\n",
    "    predd1, predd2 = dcorrect(img), dautre(img)\n",
    "    for i in range(img.shape[0]):\n",
    "        titresimg.append(\"D1 = {} | D2 = {}\".format(predd1[i], predd2[i]))\n",
    "    predd1, predd2 = dcorrect(gen.predict(img)), dautre(gen.predict(img))\n",
    "    for i in range(img.shape[0]):\n",
    "        titrestransf.append(\"D1 = {} | D2 = {}\".format(predd1[i], predd2[i]))\n",
    "\n",
    "\n",
    "    show_images(img*127.5+127.5, gen.predict(img)*127.5+127.5, titresimg, titrestransf)\n",
    "\n",
    "def save(d_A, d_B, gen_A_vers_B, gen_B_vers_A):\n",
    "    \"\"\"Sauvegarde les poids deja calculés, pour pouvoir reprendre les calculs plus tard si jamais\"\"\"\n",
    "    d_A.save_weights(\"Weights/d_A.h5\")\n",
    "    d_B.save_weights(\"Weights/d_B.h5\")\n",
    "    gen_A_vers_B.save_weights(\"Weights/gen_A_vers_B.h5\")\n",
    "    gen_B_vers_A.save_weights(\"Weights/gen_B_vers_A.h5\")\n",
    "\n",
    "def load(d_A, d_B, gen_A_vers_B, gen_B_vers_A):\n",
    "    \"\"\"Sauvegarde les poids deja calculés, pour pouvoir reprendre les calculs plus tard si jamais\"\"\"\n",
    "    if (isfile(\"Weights/d_A.h5\") and isfile(\"Weights/gen_A_vers_B.h5\") \n",
    "    and isfile(\"Weights/d_B.h5\") and isfile(\"Weights/gen_B_vers_A.h5\")):\n",
    "        d_A.load_weights(\"Weights/d_A.h5\")\n",
    "        d_B.load_weights(\"Weights/d_B.h5\")\n",
    "        gen_A_vers_B.load_weights(\"Weights/gen_A_vers_B.h5\")\n",
    "        gen_B_vers_A.load_weights(\"Weights/gen_B_vers_A.h5\")\n",
    "        print(\"Weights loaded\")\n",
    "    else:\n",
    "        print(\"Missing weights files detected. Starting from scratch\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################\n",
    "########## Let's go baby #########\n",
    "##################################\n",
    "\n",
    "dim = 128\n",
    "\n",
    "#compress_images(dim)\n",
    "#dataA, dataB = load_compressed_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XFaces,XManga = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création des discriminateur qui sont eux deja compilés\n",
    "d_Faces, d_Manga = create_discriminator(dim, name=\"Faces\"), create_discriminator(dim, name=\"Manga\")\n",
    "\n",
    "#Au tours des generateurs\n",
    "gen_F_vers_M, gen_M_vers_F = create_generator(dim, name=\"Faces_vers_Manga\"), create_generator(dim, name=\"Manga_vers_Faces\")\n",
    "\n",
    "#On charge les poids\n",
    "load(d_Faces, d_Manga, gen_F_vers_M, gen_M_vers_F)\n",
    "\n",
    "#On creer les training model\n",
    "training_model_gen_F_vers_M = create_training_model_gen(gen_F_vers_M, d_Manga, gen_M_vers_F, dim, name=\"Faces_vers_Manga\")\n",
    "training_model_gen_M_vers_F = create_training_model_gen(gen_M_vers_F, d_Faces, gen_F_vers_M, dim, name=\"Manga_vers_Faces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Et on y va\n",
    "starting_epoch = 0\n",
    "\n",
    "train(  gen_F_vers_M, d_Faces, \n",
    "        gen_M_vers_F, d_Manga, \n",
    "        training_model_gen_F_vers_M, training_model_gen_M_vers_F,  \n",
    "        XFaces, XManga, starting_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
